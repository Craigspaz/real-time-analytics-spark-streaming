{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd061fcf-32f2-4385-88e8-ad252c3b0226",
   "metadata": {},
   "source": [
    "# Real-time Impression Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee5221-d03a-450b-a672-c766ee9d10d8",
   "metadata": {},
   "source": [
    "In this sample notebook we are processing impressions from website ads and processing events in near real-time. Impressions are sent as pixel events into an Amazon Kinesis Stream. Then, events are processed and enriched within Amazon EMR using a Spark Structure Streaming application. Finally, transformed events are written directly into a realtime_clicks table in a data lake in Amazon S3.  Spark Structure Streaming works with streaming data as if you were working with regular dataframes in batch operations. It is Spark's role to figure out how to continusly read and process data making streaming development easier. For more details, see the [Sparks Structure Streaming Documentation](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html). \n",
    "\n",
    "We are using an [Apache Iceberg](https://iceberg.apache.org/) open table format to store our streamed impresions. Apache Iceberg is a high-performance format for huge analytic tables. Iceberg brings the reliability and simplicity of SQL tables to big data. To catalog tables we are using the AWS Glue Catalog to enable queries from multiple AWS native query engines like Amazon Athena or Amazon Redshift. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### PRE-REQUISITES\n",
    "\n",
    "- We assume you have an Amazon EMR cluster attached to this notebook. Testing was performed using version emr-6.15.0.\n",
    "- To simulate pixel events from a public website into Amazon Kinesis we are using the [Kinesis Data Generator tool](https://awslabs.github.io/amazon-kinesis-data-generator/web/help.html) to simulate 100s of pixel events per second. Launch the Kinesis Data Generator CloudFormation template in the same region, open the 'kinesis-data-generator' CloudFormation stack Outputs tab, and click on the KinesisDataGeneratorUrl link. Login to the generator, select the region, and select the 'emr-kinesis-stream-KinesisDataStream-xxxxxxxxxxxx' as the Stream/delivery stream. To run the code in this sample notebook, paste the following payload into the Record template tab in the Kinesis Data Generator and click 'Send Data' to simulate website ad impressions:\n",
    "\n",
    "\n",
    "```\n",
    "{        \n",
    "    \"event_time\": \"{{date.now}}\", \n",
    "    \"action_source\": \"WEBSITE\", \n",
    "    \"action_source_url\": \"www.example.com\",        \n",
    "    \"user_data\": {        \n",
    "        \"ipsrc\": \"{{random.arrayElement(\n",
    "        [\"a5b50a8b-3a77-4f83-aff4-68aa167f7c67\", \"b5b50a8b-3a77-4f83-aff4-68aa167f7c63\",\"c5b50a8b-3a77-4f83-aff4-68aa167f7c62\",\"d5b50a8b-3a77-4f83-aff4-68aa167f7c65\"]\n",
    "    )}}\",        \n",
    "        \"vmcid\": \"{{random.arrayElement(\n",
    "        [\"a5b50a8b-3a77-4f83-aff4-68aa167f7c67\", \"b5b50a8b-3a77-4f83-aff4-68aa167f7c63\",\"c5b50a8b-3a77-4f83-aff4-68aa167f7c62\",\"d5b50a8b-3a77-4f83-aff4-68aa167f7c65\"]\n",
    "    )}}\", \n",
    "        \"email\": \"{{random.arrayElement(\n",
    "        [\"a5b50a8b-3a77-4f83-aff4-68aa167f7c67\", \"b5b50a8b-3a77-4f83-aff4-68aa167f7c63\",\"c5b50a8b-3a77-4f83-aff4-68aa167f7c62\",\"d5b50a8b-3a77-4f83-aff4-68aa167f7c65\"]\n",
    "    )}}\",\n",
    "        \"region\": \"{{random.arrayElement([\"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\"ME\",\"MD\",\"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\"])}}\"\n",
    "    },\n",
    "    \"custom_data\": {        \n",
    "        \"gv\": \"12.99\",        \n",
    "        \"ec\": \"test_category\",        \n",
    "        \"el\": \"test_label\",        \n",
    "        \"ea\": \"test_action\",\n",
    "        \"cid\": 123,      \n",
    "        \"product_id\": \"{{random.arrayElement([\"product_id1\", \"product_id2\"])}}\",        \n",
    "        \"user_defined\": {         \n",
    "            \"addToCart\" : \"true\",             \n",
    "            \"signUp\" : \"true\",             \n",
    "            \"purchaseAmount\" : \"1500\"        \n",
    "        }\n",
    "}}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1638a9-250e-44d8-bde5-42233e29489e",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39526a78-0b1e-4c35-ad5d-bb9ddf2360e6",
   "metadata": {},
   "source": [
    "Configure your Spark session using the %%configure -f magic command. We will be using the Glue Catalog for Iceberg Tables. We also need to specify the jar for the [Kinesis Connector for Spark Structure Streaming](https://github.com/awslabs/spark-sql-kinesis-connector) libraries. Before you run the following step, make sure you update the S3Bucket name in the \"spark.sql.catalog.glue_catalog.warehouse\" parameter. If you choose a different bucket than the one deployed by the template, make sure your EMR cluster has read and write permissions. Search IAM roles for 'analytics-with-emr-iam-EMREC2Role'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdec5c1-e624-48f8-b652-68ddd6786bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars\": \"s3://awslabs-code-us-east-1/spark-sql-kinesis-connector/spark-streaming-sql-kinesis-connector_2.12-1.0.0.jar\",\n",
    "        \"spark.sql.catalog.glue_catalog\":\"org.apache.iceberg.spark.SparkCatalog\",\n",
    "        \"spark.sql.catalog.glue_catalog.warehouse\":\"s3://analytics-with-emr-<your AWS Account ID>/data/curated/glue_catalog/tables/\",\n",
    "        \"spark.sql.catalog.glue_catalog.catalog-impl\":\"org.apache.iceberg.aws.glue.GlueCatalog\",\n",
    "        \"spark.sql.catalog.glue_catalog.io-impl\":\"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "        \"spark.sql.extensions\":\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d01e88-c1db-45c6-9883-56c1ae450f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, to_timestamp\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39526a78-0b1e-4c35-ad5d-bb9ddf2360e5",
   "metadata": {},
   "source": [
    "Before you run the next cell open the SparkUI on the application link above to monitor your job. Also update your S3Bucket name into the checkpoint_path variable below. Finally, update the kinesis_stream_name variable to match the name of your Kinesis Data Stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af712d-c968-4701-b466-d22393523968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_url = \"https://kinesis.us-west-2.amazonaws.com\"\n",
    "kinesis_stream_name = \"<your-kinesis-stream-name>\"\n",
    "checkpoint_path = \"s3://analytics-with-emr-<your AWS Account ID>/data/checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b62796-3411-4bf8-99bc-c157001b2b60",
   "metadata": {},
   "source": [
    "## 2. Read from the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa0313-2e29-4cf2-bd74-97cec8ef60c1",
   "metadata": {},
   "source": [
    "We use the [Spark Structure Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) engine to read data from the kinesis stream using the [Kinesis Spark SQL Connector](https://github.com/awslabs/spark-sql-kinesis-connector). The connector will create a DynamoDB table for checkpointing the stream reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6a673-ddaf-44df-814e-67f6f2f097f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kinesis_stream = spark.readStream \\\n",
    "    .format(\"aws-kinesis\") \\\n",
    "    .option(\"kinesis.region\", \"us-west-2\") \\\n",
    "    .option(\"kinesis.streamName\", kinesis_stream_name) \\\n",
    "    .option(\"kinesis.consumerType\", \"GetRecords\") \\\n",
    "    .option(\"kinesis.endpointUrl\", endpoint_url) \\\n",
    "    .option(\"kinesis.startingposition\", \"LATEST\") \\\n",
    "    .option(\"kinesis.metadataCommitterType\", \"DYNAMODB\") \\\n",
    "    .option(\"kinesis.dynamodb.tableName\", \"kinesis_with_emr_checkpoints\") \\\n",
    "    .load()\n",
    "kinesis_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77990af8-6d99-42a4-80f7-4347161b8428",
   "metadata": {},
   "source": [
    "## 3. Debug code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9b369-8640-4fff-acfe-e3e49926b296",
   "metadata": {},
   "source": [
    "For developing and troubleshooting queries using interactive notebooks we can use the memory output format and validate output using Spark SQL. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the Spark application driverâ€™s memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7ca5ca-b7ea-46f8-a4cc-1067c9fc3989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = kinesis_stream.selectExpr(\"cast(data as String) as data\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='1 minute')\\\n",
    "    .queryName(\"kinesis_output\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb97617-7949-4407-b266-ac0682e645fe",
   "metadata": {},
   "source": [
    "Expect a delay on receiving data from your Amazon Kinesis Data Stream based on the time interval configurations. Wait and re-execute query if the next code returns an empty dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc733bd-3cac-48b0-9d86-aef1c1048f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = sqlContext.sql(\"SELECT * FROM kinesis_output limit 3\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52736a9-6202-44e3-a5cf-caa0dc03ee05",
   "metadata": {},
   "source": [
    "You can also run sql driectly using the \"%%sql\" command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde5b9e-8e68-46b2-bad0-3c39656929c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select count(*) from kinesis_output;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63460a06-650c-4808-8eda-a0d3c89e8f2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:09:19.697769Z",
     "iopub.status.busy": "2024-03-13T15:09:19.697540Z",
     "iopub.status.idle": "2024-03-13T15:09:19.962578Z",
     "shell.execute_reply": "2024-03-13T15:09:19.961917Z",
     "shell.execute_reply.started": "2024-03-13T15:09:19.697741Z"
    }
   },
   "source": [
    "Lets define the expected schema from your pixel payload based on the results. Spark Structure Streaming expects schema to be predefined to write the output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121587ea-754e-435d-a72d-0c72393cb326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType()),\n",
    "    StructField(\"action_source\", StringType()),\n",
    "    StructField(\"action_source_url\", StringType()),\n",
    "    StructField(\n",
    "        \"user_data\",\n",
    "        StructType([\n",
    "            StructField('ipsrc', StringType()),\n",
    "            StructField('vmcid', StringType()),\n",
    "            StructField('email', StringType()),\n",
    "            StructField('region', StringType())\n",
    "        ])),\n",
    "    StructField(\"custom_data\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3ffb9-52a7-4a14-a79c-729af6ce96eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.select(from_json(\"data\", schema).alias(\"data\")) \\\n",
    ".select(to_timestamp(\"data.event_time\").alias(\"event_time\"),\n",
    "        \"data.action_source\",\n",
    "        \"data.action_source_url\",\n",
    "        \"data.user_data.ipsrc\",\n",
    "        \"data.user_data.vmcid\",\n",
    "        \"data.user_data.email\",\n",
    "        \"data.user_data.region\",\n",
    "        \"data.custom_data\") \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498dba6b-4d2b-4288-b879-556722e81ddf",
   "metadata": {},
   "source": [
    "Once we validated our query, we can stop this application and develop the code to populate our datalake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede19b1a-a53a-4ea0-a5fd-d7dc185f924f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb922c-d183-46df-91e5-41c029141b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T22:57:01.036647Z",
     "iopub.status.busy": "2024-01-24T22:57:01.036340Z"
    },
    "tags": []
   },
   "source": [
    "## 4. Populate Data Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6049282-1651-4f2e-901d-fc2c1fbc406d",
   "metadata": {},
   "source": [
    "Once we read data from the Amazon Kinesis stream, we transform input data using Spark SQL functions and we continously write output data into an Apache Iceberg table. For more details on Iceberg as the output format of Spark Structure Streaming see [this documentation](https://iceberg.apache.org/docs/1.4.0/spark-structured-streaming/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb433d2-c480-4324-a196-2ceaeb2978d4",
   "metadata": {},
   "source": [
    "i. Create the iceberg tables in the glue catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3dfe1-5758-4111-8c3b-1f272bd58f75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "use glue_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549dffc-89da-46c1-a4f6-f683a428792e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE SCHEMA IF NOT EXISTS measurementdb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16222443-1acd-4982-a05c-12b169a4a3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE IF NOT EXISTS glue_catalog.measurementdb.realtime_clicks\n",
    "    (\n",
    "        event_time        timestamp,\n",
    "        action_source     string,\n",
    "        action_source_url string,\n",
    "        user_ipsrc        string,\n",
    "        user_vmcid        string,\n",
    "        user_email        string,\n",
    "        region            string,\n",
    "        custom_data       string\n",
    "    )\n",
    "USING iceberg\n",
    "PARTITIONED BY (date_hour(event_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4398fd-4f42-4826-859c-1b183a5b9871",
   "metadata": {},
   "source": [
    "ii. Use Spark Structure Streaming to continously write data into the Iceberg table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40d9a1-2579-48d6-be39-de648af0dea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType()),\n",
    "    StructField(\"action_source\", StringType()),\n",
    "    StructField(\"action_source_url\", StringType()),\n",
    "    StructField(\"user_data\", StructType([\n",
    "            StructField('ipsrc', StringType()),\n",
    "            StructField('vmcid', StringType()),\n",
    "            StructField('email', StringType()),\n",
    "            StructField('region', StringType())\n",
    "        ])),\n",
    "    StructField(\"custom_data\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469f405-5845-4b99-96cd-aed866e15398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kinesis_stream.select(from_json(col(\"data\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "    .select(to_timestamp(\"data.event_time\").alias(\"event_time\")\n",
    "            ,col(\"data.action_source\").alias(\"action_source\")\n",
    "            ,col(\"data.action_source_url\").alias(\"action_source_url\")\n",
    "            ,col(\"data.user_data.ipsrc\").alias(\"user_ipsrc\")\n",
    "            ,col(\"data.user_data.vmcid\").alias(\"user_vmcid\")\n",
    "            ,col(\"data.user_data.email\").alias(\"user_email\")\n",
    "            ,col(\"data.user_data.region\").alias(\"region\")\n",
    "            ,col(\"data.custom_data\").alias(\"custom_data\")) \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"realtime_clicks\") \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"fanout-enabled\", \"true\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .trigger(processingTime='1 minute') \\\n",
    "    .toTable(\"measurementdb.realtime_clicks\") \\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c0a7b-0d35-4a48-8e88-bd808d4d14d4",
   "metadata": {},
   "source": [
    "The execution will remain in progress until the application is terminated. Login to the [Amazon Athena Console](https://us-west-2.console.aws.amazon.com/athena) to query the \"measurmentdb.realtime_clicks\" table. Query example: SELECT * FROM \"AwsDataCatalog\".\"measurementdb\".\"realtime_clicks\"limit 10;\n",
    "\n",
    "As an alternative you can also use a different notebook to run this queries in your table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee48a3-4ff4-40fb-a150-c87e101edb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM measurementdb.realtime_clicks limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e546022-e424-43a6-8f90-76e18466b1c7",
   "metadata": {},
   "source": [
    "Run the following query couple times across 1 minute intervals to see how record counts increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1665a-7cae-4667-b73e-316af76bf355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT count(*) FROM measurementdb.realtime_clicks;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb9561-7bf0-4ec5-9580-965e2f98f721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T20:01:17.776019Z",
     "iopub.status.busy": "2024-01-29T20:01:17.775788Z"
    }
   },
   "source": [
    "## 5. Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd377e4-d3bc-4240-adf6-9fd96b50e18b",
   "metadata": {},
   "source": [
    "Expect data lakes to produce a high number of small files (Kbs) when they are populated from streaming data sources. In addition, Iceberg tables create a new snapshot, or version, of a table for each data write. Iceberg snapshots allow tables to be rolled back to any prior valid snapshots in the event of an error. To optimize query performance and storage cost, Apache Iceberg and the AWS Glue Catalog provide abstractions and features to delete old snapshots and merge files into larger objects. Consider implementing workloads for periodic maintenance of your tables. For more information on this topic see [Apache Iceberg Maintenance Documentation](https://iceberg.apache.org/docs/1.2.0/maintenance/), [AWS Glue Automatic Compaction of Apache Iceberg Tables](https://aws.amazon.com/blogs/aws/aws-glue-data-catalog-now-supports-automatic-compaction-of-apache-iceberg-tables/) and the [Optimizing Iceberg Tables in Athena Documentation](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-data-optimization.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b610f9-b821-409a-b0b9-c66cb7c0a8af",
   "metadata": {},
   "source": [
    "i. See the latest snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421944fc-1879-48b9-9d41-596885c86e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM measurementdb.realtime_clicks.snapshots limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd399e-f5ca-48bf-bf56-ab74e710bbc4",
   "metadata": {},
   "source": [
    "ii. Expire older snapshots. Update the date to a value greater than the latest snapshot. After running the command, execute the previous statement to see how snapshots are reduced to the last 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9deda-b1e7-482d-be7f-655963e8b54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"CALL glue_catalog.system.expire_snapshots(table => 'glue_catalog.measurementdb.realtime_clicks', older_than => DATE '2024-02-13', retain_last => 2)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d63fb-89e8-4f07-a7e6-f8aaf8db6199",
   "metadata": {},
   "source": [
    "ii. Automatic data compaction can be enabled through the AWS console or programatically with AWS Glue APIs or via Athena. For more details on the process, see the post for [AWS Glue Automatic Compaction of Apache Iceberg Tables](https://aws.amazon.com/blogs/aws/aws-glue-data-catalog-now-supports-automatic-compaction-of-apache-iceberg-tables/) or [Optimizing Iceberg Tables in Athena Documentation](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-data-optimization.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc5fee-1c40-4f21-ab7c-bc059b60e7c9",
   "metadata": {},
   "source": [
    "## 6. Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06740d1-013a-4fed-844d-0e6d3e86eac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS measurementdb.realtime_clicks;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
